{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T12:16:08.431084Z",
     "start_time": "2019-06-27T12:16:08.423218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas, matplotlib, random\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T09:41:06.217387Z",
     "start_time": "2019-06-28T09:41:06.197050Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_K = 10 # default number of folds\n",
    "\n",
    "def holisticsTrainTest(dataframe, list_complete_participants, train_test_split = 0.8):\n",
    "    \"\"\"\n",
    "    Prepare a dataframe and split it into 70% training and 30% test. Return both sets.\n",
    "    It's assumed the dataframe does have the participant_no feature\n",
    "    \"\"\"\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "\n",
    "    random.seed(75)\n",
    "    random.shuffle(list_complete_participants)\n",
    "    random.seed(75)\n",
    "    test_participants = random.sample(set(list_complete_participants), \n",
    "                                      int(round((1 - train_test_split) * len(list_complete_participants))))\n",
    "#     print(test_participants)\n",
    "    print(len(test_participants))\n",
    "\n",
    "    # only pick the 30% of the complete participants for testing\n",
    "    df_test = df[df['Participant_No'].isin(test_participants)]\n",
    "\n",
    "    print(\"Testing on participants:\")\n",
    "    print(df_test['Participant_No'].unique())\n",
    "    \n",
    "    # use the rest for training (the negate of above)\n",
    "    df_train = df[~df['Participant_No'].isin(test_participants)]\n",
    "        \n",
    "    # removing the participant number since it's a holistic model\n",
    "#     del df['Participant_No']\n",
    "    del df_test['Participant_No']\n",
    "    del df_train['Participant_No']\n",
    "         \n",
    "    # shuffle \n",
    "    df_train = df_train.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "    \n",
    "#     # determine split\n",
    "#     idx_split = int(df.shape[0] * train_test_split)\n",
    "    \n",
    "#     # split the dataframe\n",
    "#     df_train = df.iloc[:idx_split, :]\n",
    "#     df_test = df.iloc[idx_split:, :]\n",
    "    \n",
    "    # create binary label versions of the sets\n",
    "    df_train_binary = df_train.copy()\n",
    "    df_test_binary = df_test.copy()\n",
    "    df_train_binary['Discrete Thermal Comfort_TA'] = df_train['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    df_test_binary['Discrete Thermal Comfort_TA'] = df_test['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    return df_train, df_test, df_train_binary, df_test_binary\n",
    "\n",
    "def LOPOTrainTest(dataframe, list_participants_65, train_test_split = 0.8, with_labels=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataframes and split into 80% of participants for training and 20% for testing. However, technically\n",
    "    there are a bit more than 65 participants in the dataframe but we are only using 65 as the number for train-test\n",
    "    split. Thus, the remaining participants, with fewer responses, will be used for training purposes, and will also\n",
    "    be ignored for cv later on\n",
    "    \"\"\"\n",
    "\n",
    "    test_participants = random.sample(set(list_participants_65), int(round((1- train_test_split) * len(list_participants_65))))\n",
    "\n",
    "    # only pick the 20% of the 65 participants for testing\n",
    "    df_test = dataframe[dataframe['Participant_No'].isin(test_participants)]\n",
    "    # use the rest for training (the negate of above)\n",
    "    df_train = dataframe[~dataframe['Participant_No'].isin(test_participants)]\n",
    "\n",
    "    if with_labels:\n",
    "        X_train = np.array(df_train.iloc[:, 0:df_train.shape[1] - 1]) # minus 1 for the comfort label\n",
    "        y_train = np.array(df_train['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "        X_test = np.array(df_test.iloc[:, 0:df_test.shape[1] - 1]) # minus 1 for the comfort label\n",
    "        y_test = np.array(df_test['Discrete Thermal Comfort_TA'])\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        return df_train, df_test\n",
    "\n",
    "def LOPO_cv(X, list_participants_65):\n",
    "    \"\"\"\n",
    "    Workst as a cross-validation generator. It returns an iterable of length n_folds, each element of which is a \n",
    "    2-tuple of numpy 1-d arrays (train_index, test_index) containing the indices of the test and training sets for \n",
    "    that cross-validation run. \n",
    "    The number of folds will be equal to the number of participants in dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sure the splits are only based on participants with complete data\n",
    "    existing_p_in_X = list(set(X[:, 0]).intersection(list_participants_65))\n",
    "    n_folds = len(existing_p_in_X)\n",
    "    \n",
    "    # CV array of 2-tuples\n",
    "    lopo_CViterator = []\n",
    "    \n",
    "    # auxiliary dataframe just so we can get the indices later\n",
    "    aux_df = pd.DataFrame(X)\n",
    "    \n",
    "    # TODO: DEBUGGING\n",
    "#     print(existing_p_in_X)\n",
    "#     print(aux_df)\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        # use current participant to separate as the test set\n",
    "        curr_participant = existing_p_in_X[i]\n",
    "        test_indices = aux_df[aux_df.iloc[:,0] == curr_participant].index.values.astype(int)\n",
    "        # the rest participants will be for train\n",
    "        train_indices = aux_df[aux_df.iloc[:,0] != curr_participant].index.values.astype(int)\n",
    "\n",
    "        #TODO: DEBUGGING\n",
    "#         print(test_idx)\n",
    "#         print(train_idx)\n",
    "\n",
    "        # append both indices to the iterable\n",
    "        lopo_CViterator.append( (train_indices, test_indices) )\n",
    "    \n",
    "    return lopo_CViterator\n",
    "\n",
    "def predictions_to_temp(predictions, df_train, df_test):\n",
    "    temp_range_train = [ df_train['Temperature (Fahrenheit)'].min(), df_train['Temperature (Fahrenheit)'].max()]\n",
    "    temp_range_test = [ df_test['Temperature (Fahrenheit)'].min(), df_test['Temperature (Fahrenheit)'].max()]\n",
    "    return\n",
    "\n",
    "\n",
    "def chooseK(train_labels):\n",
    "    \"\"\"\n",
    "    Determine number of folds\n",
    "    \"\"\"\n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    classCounter = Counter(train_labels)\n",
    "    numLeastCommonClass = min(classCounter.values())\n",
    "    return min(numLeastCommonClass, DEFAULT_K)\n",
    "\n",
    "def getClfMetrics(test_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Compute different validation metrics for a classification problem.\n",
    "    Metrics:\n",
    "    - micro and macro F1 score\n",
    "    - Confusion Matrix\n",
    "    - Classification Report\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = accuracy_score(test_labels, pred_labels) \n",
    "    print(\"\\nAccuracy (f1 micro) on test set: \", acc)\n",
    "    print(\"F1 micro on test set: \", f1_score(test_labels, pred_labels, average = 'micro'))\n",
    "    print(\"F1 macro on test set: \", f1_score(test_labels, pred_labels, average = 'macro'))\n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "    print(confusion_matrix(test_labels, pred_labels)) #, labels = [-2, -1, 0, 1, 2])\n",
    "    print(\"\\nClassification Metrics: \")\n",
    "    print(classification_report(test_labels, pred_labels))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T09:42:00.019958Z",
     "start_time": "2019-06-28T09:41:59.986227Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateCost(dataframe, list_complete_participants, temp_high, temp_low, clf=None, occuTherm=False):\n",
    "    \n",
    "    train_test_split = 0.8\n",
    "\n",
    "    random.seed(75)\n",
    "    random.shuffle(list_complete_participants)\n",
    "    random.seed(75)\n",
    "    test_participants = random.sample(set(list_complete_participants), \n",
    "                                      int(round((1 - train_test_split) * len(list_complete_participants))))\n",
    "    \n",
    "    \n",
    "    # take the same 30% used for data split\n",
    "    df_test = dataframe[dataframe['Participant_No'].isin(test_participants)]\n",
    "\n",
    "    # the temperature range we care about is such that all the baselines predicts comfortable\n",
    "    pred_comfort = 0\n",
    "    \n",
    "    # get list of participants in the test set\n",
    "    participants = list(df_test['Participant_No'].unique())\n",
    "\n",
    "    print(\"Testing on participants:\")\n",
    "    print(df_test['Participant_No'].unique())\n",
    "        \n",
    "    accumulative_rmse = 0 # accumulative RMSE\n",
    "    total_rmse = 0\n",
    "    total_num_responses = 0\n",
    "    \n",
    "    # for each participant\n",
    "    for p in participants:\n",
    "        curr_participant = df_test[df_test['Participant_No'] == p]\n",
    "        \n",
    "        # RMSE calculation variables\n",
    "        participant_rmse = 0 \n",
    "        participant_num_respones = 0\n",
    "\n",
    "        # if we are evaluating our model, we need to find the comfort temp range for each participant\n",
    "        if occuTherm:\n",
    "            curr_p_occutherm = curr_participant.copy()\n",
    "            \n",
    "            # for prediction we don't use these columns\n",
    "            del curr_p_occutherm['Discrete Thermal Comfort_TA']\n",
    "            del curr_p_occutherm['Participant_No']\n",
    "            \n",
    "            # calculate the range of temperature at which occuTherm model predicts 0\n",
    "            \n",
    "            occutherm_pred = clf.predict(curr_p_occutherm)\n",
    "                        \n",
    "            occutherm_temps = []\n",
    "            curr_p_occutherm = curr_p_occutherm.reset_index(drop=True)\n",
    "            \n",
    "            for index, row in curr_p_occutherm.iterrows():\n",
    "                temp = row['Temperature (Fahrenheit)']\n",
    "                if occutherm_pred[index] == 0:\n",
    "                    occutherm_temps.append(temp)\n",
    "        \n",
    "            temp_low = min(occutherm_temps)  # fahrenheit\n",
    "            temp_high = max(occutherm_temps) # fahrenheit\n",
    "\n",
    "            print(temp_low)\n",
    "            print(temp_high)\n",
    "            \n",
    "        # iterate through all the participant's responses\n",
    "        for index, row in curr_participant.iterrows():\n",
    "            temp = row['Temperature (Fahrenheit)']\n",
    "            true_comfort = row['Discrete Thermal Comfort_TA']\n",
    "            \n",
    "            # see if temperature falls within range of the baseline\n",
    "            if (temp >= temp_low) and (temp <= temp_high):\n",
    "                # keep track of current participant's cost\n",
    "                participant_rmse = participant_rmse + (pred_comfort - true_comfort) ** 2\n",
    "                print((pred_comfort - true_comfort)**2)\n",
    "                participant_num_respones = participant_num_respones + 1\n",
    "                \n",
    "        # end of current participant's responses\n",
    "        \n",
    "        # continue to add with the following participants\n",
    "        if participant_num_respones != 0:\n",
    "            total_rmse = total_rmse + participant_rmse # add\n",
    "            total_num_responses = total_num_responses + participant_num_respones\n",
    "            # rmse for current participant\n",
    "            participant_rmse = math.sqrt(participant_rmse / participant_num_respones)\n",
    "\n",
    "        # add participants RMSE\n",
    "        accumulative_rmse = accumulative_rmse + participant_rmse\n",
    "\n",
    "        # calculate rmse of current participant\n",
    "        print(\"Num Participants responses: {}\".format(curr_participant.shape[0]))\n",
    "\n",
    "\n",
    "    # all participants processed\n",
    "    total_rmse = math.sqrt(total_rmse / total_num_responses)\n",
    "    \n",
    "    print(\"Total RMSE across all participants: {}\".format(total_rmse))\n",
    "    print(\"Accumulative of RMSE of each participant: {}\".format(accumulative_rmse))\n",
    "    \n",
    "    return total_rmse, accumulative_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.590669Z",
     "start_time": "2019-06-27T07:13:45.574937Z"
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/CenterForTheBuiltEnvironment/comfort_tool/blob/master/contrib/comfort_models.py\n",
    "def comfPMV(ta, tr, vel, rh, met, clo, wme):\n",
    "    \"\"\"\n",
    "    returns [pmv, ppd]\n",
    "    ta, air temperature (C)\n",
    "    tr, mean radiant temperature (C)\n",
    "    vel, relative air velocity (m/s)\n",
    "    rh, relative humidity (%) Used only this way to input humidity level\n",
    "    met, metabolic rate (met)\n",
    "    clo, clothing (clo)\n",
    "    wme, external work, normally around 0 (met)\n",
    "    \"\"\"\n",
    "\n",
    "    pa = rh * 10 * math.exp(16.6536 - 4030.183 / (ta + 235))\n",
    "\n",
    "    icl = 0.155 * clo  # thermal insulation of the clothing in M2K/W\n",
    "    m = met * 58.15  # metabolic rate in W/M2\n",
    "    w = wme * 58.15  # external work in W/M2\n",
    "    mw = m - w  # internal heat production in the human body\n",
    "    if (icl <= 0.078):\n",
    "        fcl = 1 + (1.29 * icl)\n",
    "    else:\n",
    "        fcl = 1.05 + (0.645 * icl)\n",
    "\n",
    "    # heat transf. coeff. by forced convection\n",
    "    hcf = 12.1 * math.sqrt(vel)\n",
    "    taa = ta + 273\n",
    "    tra = tr + 273\n",
    "    tcla = taa + (35.5 - ta) / (3.5 * icl + 0.1)\n",
    "\n",
    "    p1 = icl * fcl\n",
    "    p2 = p1 * 3.96\n",
    "    p3 = p1 * 100\n",
    "    p4 = p1 * taa\n",
    "    p5 = (308.7 - 0.028 * mw) + (p2 * math.pow(tra / 100, 4))\n",
    "    xn = tcla / 100\n",
    "    xf = tcla / 50\n",
    "    eps = 0.00015\n",
    "\n",
    "    n = 0\n",
    "    while abs(xn - xf) > eps:\n",
    "        xf = (xf + xn) / 2\n",
    "        hcn = 2.38 * math.pow(abs(100.0 * xf - taa), 0.25)\n",
    "        if (hcf > hcn):\n",
    "            hc = hcf\n",
    "        else:\n",
    "            hc = hcn\n",
    "        xn = (p5 + p4 * hc - p2 * math.pow(xf, 4)) / (100 + p3 * hc)\n",
    "        n += 1\n",
    "        if (n > 150):\n",
    "            print('Max iterations exceeded')\n",
    "            return 1\n",
    "\n",
    "\n",
    "    tcl = 100 * xn - 273\n",
    "\n",
    "    # heat loss diff. through skin\n",
    "    hl1 = 3.05 * 0.001 * (5733 - (6.99 * mw) - pa)\n",
    "    # heat loss by sweating\n",
    "    if mw > 58.15:\n",
    "        hl2 = 0.42 * (mw - 58.15)\n",
    "    else:\n",
    "        hl2 = 0\n",
    "    # latent respiration heat loss\n",
    "    hl3 = 1.7 * 0.00001 * m * (5867 - pa)\n",
    "    # dry respiration heat loss\n",
    "    hl4 = 0.0014 * m * (34 - ta)\n",
    "    # heat loss by radiation\n",
    "    hl5 = 3.96 * fcl * (math.pow(xn, 4) - math.pow(tra / 100, 4))\n",
    "    # heat loss by convection\n",
    "    hl6 = fcl * hc * (tcl - ta)\n",
    "\n",
    "    ts = 0.303 * math.exp(-0.036 * m) + 0.028\n",
    "    pmv = ts * (mw - hl1 - hl2 - hl3 - hl4 - hl5 - hl6)\n",
    "    ppd = 100.0 - 95.0 * math.exp(-0.03353 * pow(pmv, 4.0)\n",
    "        - 0.2179 * pow(pmv, 2.0))\n",
    "\n",
    "    r = []\n",
    "    r.append(pmv)\n",
    "    r.append(ppd)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.602594Z",
     "start_time": "2019-06-27T07:13:45.591845Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppv(pmv, dataframe_train, dataframe_test, ta, tr, vel, rh, met, clo, binary=False, no_clo=False):\n",
    "    \"\"\"\n",
    "    returns ppv as a linear combination of the following features:\n",
    "    ta, air temperature (C)\n",
    "    tr, mean radiant temperature (C)\n",
    "    vel, relative air velocity (m/s)\n",
    "    rh, relative humidity (%) Used only this way to input humidity level\n",
    "    met, metabolic rate (met)\n",
    "    clo, clothing (clo)\n",
    "    \n",
    "    According to SPOT paper:\n",
    "    ppv(x) = pmv(x) + personal(x)\n",
    "\n",
    "    personal(x) = a'.x + b\n",
    "\n",
    "    a={a_temp, a_radiant, a_velocity, a_humidity, a_metabolic, a_clothing}\n",
    "    \"\"\"\n",
    "    \n",
    "    ppv_train_df = pd.DataFrame()\n",
    "    ppv_test_df = pd.DataFrame()\n",
    "    \n",
    "    ppv_train_df['Temperature (Fahrenheit)'] = dataframe_train['Temperature (Fahrenheit)']\n",
    "    ppv_train_df['tr'] = 25\n",
    "    ppv_train_df['vel'] = 0.2\n",
    "    ppv_train_df['rh'] = 0.6\n",
    "    ppv_train_df['met'] = 1.1\n",
    "    ppv_train_df['Discrete Thermal Comfort_TA'] = dataframe_train['Discrete Thermal Comfort_TA']\n",
    "\n",
    "    ppv_test_df['Temperature (Fahrenheit)'] = dataframe_test['Temperature (Fahrenheit)']\n",
    "    ppv_test_df['tr'] = 25\n",
    "    ppv_test_df['vel'] = 0.2\n",
    "    ppv_test_df['rh'] = 0.6\n",
    "    ppv_test_df['met'] = 1.1\n",
    "    ppv_test_df['Discrete Thermal Comfort_TA'] = dataframe_test['Discrete Thermal Comfort_TA']\n",
    "        \n",
    "    if no_clo:\n",
    "        ppv_train_df['ClothingInsulation'] = 0.8\n",
    "        ppv_test_df['ClothingInsulation'] = 0.8\n",
    "    else:\n",
    "        ppv_train_df['ClothingInsulation'] = dataframe_train['ClothingInsulation']\n",
    "        ppv_test_df['ClothingInsulation'] = dataframe_test['ClothingInsulation']\n",
    "    \n",
    "    # remapping for binary \n",
    "    if binary:\n",
    "        ppv_train_df['Discrete Thermal Comfort_TA'] = ppv_train_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        ppv_test_df['Discrete Thermal Comfort_TA'] = ppv_test_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        \n",
    "    # break down the df\n",
    "    X_train = np.array(ppv_train_df.iloc[:, 0:ppv_train_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_train = np.array(ppv_train_df['Discrete Thermal Comfort_TA'])\n",
    "    \n",
    "    X_test = np.array(ppv_test_df.iloc[:, 0:ppv_test_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_test = np.array(ppv_test_df['Discrete Thermal Comfort_TA'])\n",
    "        \n",
    "    # train linear regression\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    personal_pred = reg.predict(X_test)\n",
    "    \n",
    "    ppv_pred = np.around(pmv + personal_pred, 0)\n",
    "    \n",
    "    # clip to -2,+2\n",
    "    ppv_pred = np.clip(ppv_pred, a_min = -2, a_max = 2) \n",
    "    \n",
    "    # clip to 0,1\n",
    "    if binary:\n",
    "        ppv_pred = np.clip(ppv_pred, a_min = 0, a_max = 1) \n",
    "    \n",
    "    # get metrics\n",
    "    return getClfMetrics(y_test, ppv_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.619912Z",
     "start_time": "2019-06-27T07:13:45.603775Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppv_value(pmv, dataframe_train, dataframe_test, ta, tr, vel, rh, met, clo, binary=False, no_clo=False):\n",
    "    \"\"\"\n",
    "    returns ppv as a linear combination of the following features:\n",
    "    ta, air temperature (C)\n",
    "    tr, mean radiant temperature (C)\n",
    "    vel, relative air velocity (m/s)\n",
    "    rh, relative humidity (%) Used only this way to input humidity level\n",
    "    met, metabolic rate (met)\n",
    "    clo, clothing (clo)\n",
    "    \n",
    "    According to SPOT paper:\n",
    "    ppv(x) = pmv(x) + personal(x)\n",
    "\n",
    "    personal(x) = a'.x + b\n",
    "\n",
    "    a={a_temp, a_radiant, a_velocity, a_humidity, a_metabolic, a_clothing}\n",
    "    \"\"\"\n",
    "    \n",
    "    ppv_train_df = pd.DataFrame()\n",
    "    ppv_test_df = pd.DataFrame()\n",
    "    \n",
    "    ppv_train_df['Temperature (Fahrenheit)'] = dataframe_train['Temperature (Fahrenheit)']\n",
    "    ppv_train_df['tr'] = 25\n",
    "    ppv_train_df['vel'] = 0.2\n",
    "    ppv_train_df['rh'] = 0.6\n",
    "    ppv_train_df['met'] = 1.1\n",
    "    ppv_train_df['Discrete Thermal Comfort_TA'] = dataframe_train['Discrete Thermal Comfort_TA']\n",
    "\n",
    "    ppv_test_df['Temperature (Fahrenheit)'] = dataframe_test['Temperature (Fahrenheit)']\n",
    "    ppv_test_df['tr'] = 25\n",
    "    ppv_test_df['vel'] = 0.2\n",
    "    ppv_test_df['rh'] = 0.6\n",
    "    ppv_test_df['met'] = 1.1\n",
    "    ppv_test_df['Discrete Thermal Comfort_TA'] = dataframe_test['Discrete Thermal Comfort_TA']\n",
    "        \n",
    "    if no_clo:\n",
    "        ppv_train_df['ClothingInsulation'] = 0.8\n",
    "        ppv_test_df['ClothingInsulation'] = 0.8\n",
    "    else:\n",
    "        ppv_train_df['ClothingInsulation'] = dataframe_train['ClothingInsulation']\n",
    "        ppv_test_df['ClothingInsulation'] = dataframe_test['ClothingInsulation']\n",
    "    \n",
    "    # remapping for binary \n",
    "    if binary:\n",
    "        ppv_train_df['Discrete Thermal Comfort_TA'] = ppv_train_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        ppv_test_df['Discrete Thermal Comfort_TA'] = ppv_test_df['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "        \n",
    "    # break down the df\n",
    "    X_train = np.array(ppv_train_df.iloc[:, 0:ppv_train_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_train = np.array(ppv_train_df['Discrete Thermal Comfort_TA'])\n",
    "    \n",
    "    X_test = np.array(ppv_test_df.iloc[:, 0:ppv_test_df.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_test = np.array(ppv_test_df['Discrete Thermal Comfort_TA'])\n",
    "        \n",
    "    # train linear regression\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    personal_pred = reg.predict(X_test) # we want the ppv value or the current training\n",
    "    \n",
    "    ppv_pred = pmv + personal_pred\n",
    "    \n",
    "    # clip to -2,+2\n",
    "    ppv_pred = np.clip(ppv_pred, a_min = -2, a_max = 2) \n",
    "    \n",
    "    # clip to 0,1\n",
    "    if binary:\n",
    "        ppv_pred = np.clip(ppv_pred, a_min = 0, a_max = 1) \n",
    "    \n",
    "    return ppv_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.636734Z",
     "start_time": "2019-06-27T07:13:45.621069Z"
    }
   },
   "outputs": [],
   "source": [
    "def selectModelParameters(train_vectors, train_labels, trainclf, parameters, scorer, useSampleWeight = False, \n",
    "                          list_participants_65 = None, LOPO=False):\n",
    "    \"\"\"\n",
    "    Choose the best combination of parameters for a given model\n",
    "    \"\"\"\n",
    "    \n",
    "    k = chooseK(train_labels) # get number of folds\n",
    "\n",
    "    stratifiedKFold = StratifiedKFold(n_splits = k)\n",
    "    if useSampleWeight:\n",
    "        n_samples = len(train_labels)\n",
    "        n_classes = len(set(train_labels))\n",
    "        classCounter = Counter(train_labels)\n",
    "        sampleWeights = [n_samples / (n_classes * classCounter[label]) for label in train_labels]\n",
    "        \n",
    "        if LOPO:\n",
    "            chosen_cv = LOPO_cv(train_vectors, list_participants_65)\n",
    "            # train_vectors still have the Participant_No, so remove the first column\n",
    "            train_vectors = train_vectors[:,1:]\n",
    "            print(\"Number of folds: \" + str(len(chosen_cv)))\n",
    "        else:\n",
    "            print(\"Number of folds: \" + str(k))\n",
    "            chosen_cv = stratifiedKFold\n",
    "        \n",
    "        gridSearch = GridSearchCV(trainclf, parameters, cv = chosen_cv, scoring = scorer, \n",
    "                                      fit_params = {'sample_weight' : sampleWeights})\n",
    "    else:\n",
    "        if LOPO:\n",
    "            chosen_cv = LOPO_cv(train_vectors, list_participants_65)\n",
    "            # train_vectors still have the Participant_No, so remove the first column\n",
    "            train_vectors = train_vectors[:,1:]\n",
    "            print(\"Number of folds: \" + str(len(chosen_cv)))\n",
    "        else:\n",
    "            print(\"Number of folds: \" + str(k))\n",
    "            chosen_cv = stratifiedKFold\n",
    "        \n",
    "        gridSearch = GridSearchCV(trainclf, parameters, cv = chosen_cv, scoring = scorer)\n",
    "    \n",
    "    gridSearch.fit(train_vectors, train_labels)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(gridSearch.best_params_)\n",
    "    \n",
    "    return gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.654518Z",
     "start_time": "2019-06-27T07:13:45.638429Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainTest_tunedModel(df_train, df_test, clf_optimal, LOPO=False, mlp_tuned=False, binary_mlp=False):\n",
    "    # transform unseen test set and whole train split\n",
    "    X_train = np.array(df_train.iloc[:, 0:df_train.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_train = np.array(df_train['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    X_test = np.array(df_test.iloc[:, 0:df_test.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_test = np.array(df_test['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    # remapping for mlp\n",
    "    if mlp_tuned:\n",
    "        # re-map labels from {-2, -1, 0, 1, 2} to {0, 1, 2, 3, 4}; works better with _tanh_ activation\n",
    "        y_norm_train = y_train + 2\n",
    "        y_norm_test = y_test + 2\n",
    "    \n",
    "        # binary case\n",
    "        if binary_mlp:\n",
    "            y_norm_train = df_train['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "            y_norm_test = df_test['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "        # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "        y_train = np.zeros((y_norm_train.size, y_norm_train.max()+1)) \n",
    "        y_train[np.arange(y_norm_train.size),y_norm_train] = 1 \n",
    "\n",
    "        y_test = np.zeros((y_norm_test.size, y_norm_test.max()+1)) \n",
    "        y_test[np.arange(y_norm_test.size),y_norm_test] = 1 \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train) # TODO: does this take care of boolean variables?\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        \n",
    "    if LOPO:\n",
    "        # X datasets still have the Participant_No, so remove the first column\n",
    "        X_train = X_train[:,1:]\n",
    "        X_test = X_test[:,1:]\n",
    "        \n",
    "    # retrain in all train split with the tuned model\n",
    "    clf_optimal.fit(X_train, y_train)\n",
    "\n",
    "    #predict the response on test set\n",
    "    y_pred = clf_optimal.predict(X_test)\n",
    "\n",
    "    if mlp_tuned:\n",
    "        # reverse one hot encoding\n",
    "        y_pred_normal = []\n",
    "        y_test_normal = []\n",
    "\n",
    "        if binary_mlp:\n",
    "            for i in range(0,len(y_test)):\n",
    "                if y_pred[i,0] == 1:\n",
    "                    y_pred_normal.append(0)\n",
    "                elif y_pred[i,1] == 1:\n",
    "                    y_pred_normal.append(1)\n",
    "                else:\n",
    "                    y_pred_normal.append(0)\n",
    "                    \n",
    "                if y_test[i,0] == 1:\n",
    "                    y_test_normal.append(0)\n",
    "                elif y_test[i,1] ==1:\n",
    "                    y_test_normal.append(1)\n",
    "                else:\n",
    "                    y_pred_normal.append(0)\n",
    "\n",
    "        else: \n",
    "            for i in range(0,len(y_test)):\n",
    "                if y_pred[i,0] == 1:\n",
    "                    y_pred_normal.append(-2)\n",
    "                elif y_pred[i,1] == 1:\n",
    "                    y_pred_normal.append(-1)\n",
    "                elif y_pred[i,2] == 1:\n",
    "                    y_pred_normal.append(0)\n",
    "                elif y_pred[i,3] == 1:\n",
    "                    y_pred_normal.append(1)\n",
    "                elif y_pred[i,4] == 1:\n",
    "                    y_pred_normal.append(2)\n",
    "                else:\n",
    "                    y_pred_normal.append(0)\n",
    "\n",
    "                if y_test[i,0] == 1:\n",
    "                    y_test_normal.append(-2)\n",
    "                elif y_test[i,1] ==1:\n",
    "                    y_test_normal.append(-1)\n",
    "                elif y_test[i,2] == 1:\n",
    "                    y_test_normal.append(0)\n",
    "                elif y_test[i,3] == 1:\n",
    "                    y_test_normal.append(1)\n",
    "                elif y_test[i,4] == 1:\n",
    "                    y_test_normal.append(2)\n",
    "                else:\n",
    "                    y_pred_normal.append(0)\n",
    "    \n",
    "\n",
    "        y_pred= np.array(y_pred_normal)\n",
    "        y_test = np.array(y_test_normal)\n",
    "\n",
    "    \n",
    "    # get metrics\n",
    "    return getClfMetrics(y_test, y_pred), y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.671765Z",
     "start_time": "2019-06-27T07:13:45.655861Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainRF(dataframe, list_participants_65 = None, LOPO=False):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and then find the optimal tree depth and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    parameters = {'n_estimators' : [10, 100, 1000],\n",
    "                  'criterion' : ['entropy', 'gini'],\n",
    "                  'min_samples_split' : [2, 10, 20, 30], \n",
    "                  'class_weight' : ['balanced', 'balanced_subsample']}\n",
    "    scorer = 'f1_micro'\n",
    "    clf = RandomForestClassifier(n_estimators = 10, min_samples_split = 2, class_weight = 'balanced', \n",
    "                                 random_state = 100)\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # Leave-one-person-out cross-validation (LOPO)\n",
    "    # --------------------------------------------\n",
    "    if LOPO:\n",
    "        # split into train and test (they still contain the Participant_No as a column)\n",
    "        X_train, X_test, y_train, y_test = LOPOTrainTest(dataframe, list_participants_65, with_labels=True)\n",
    "        rf_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer, \n",
    "                                              list_participants_65=list_participants_65, LOPO=True)\n",
    "\n",
    "        # find optimal depth and generate model\n",
    "        optimal_depth = chooseOptimalTreeDepth(rf_classifier, X_train, y_train, \n",
    "                                               list_participants_65=list_participants_65, LOPO=True)\n",
    "        \n",
    "        # X datasets still have the Participant_No, so remove the first column\n",
    "        X_train = X_train[:,1:]\n",
    "        X_test = X_test[:,1:]\n",
    "\n",
    "    else:\n",
    "         # split into train and test CV\n",
    "        test_size_percentage = 0.2\n",
    "\n",
    "        # X_train = train + cv set (train_vectors)\n",
    "        # X_test = test set (test_vectors)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size_percentage, \n",
    "                                                        random_state = 100, stratify = y)\n",
    "        # for the holistic approach, dataframe doesn't have the Participants_No column so proceed to do gridsearchCV\n",
    "        rf_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "        \n",
    "        # find optimal depth and generate model\n",
    "        optimal_depth = chooseOptimalTreeDepth(rf_classifier, X_train, y_train)\n",
    "\n",
    "    # generate the model with the selected paramters plus the optimal depth and do the model fitting\n",
    "    rf_optimal = rf_classifier.set_params(max_depth = optimal_depth)\n",
    "    print(rf_optimal)\n",
    "    \n",
    "    # fit the model\n",
    "    rf_optimal.fit(X_train, y_train)\n",
    "\n",
    "    # predict the response on test set\n",
    "    y_pred = rf_optimal.predict(X_test)\n",
    "\n",
    "    # get metrics\n",
    "    rf_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return rf_acc, rf_optimal\n",
    "\n",
    "# Choose the optimal depth of a tree model \n",
    "def chooseOptimalTreeDepth(clf, train_vectors, train_labels, list_participants_65 = None,\n",
    "                           LOPO=False, plot = True):\n",
    "    \"\"\"\n",
    "    Choose the optimal depth of a tree model \n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # generate a list of potential depths to calculate the optimal\n",
    "    depths = list(range(1, 25))\n",
    "\n",
    "    # empty list that will hold cv scores\n",
    "    cv_scores = []\n",
    "\n",
    "    print(\"Finding optimal tree depth\")\n",
    "    # find optimal tree depth    \n",
    "    for d in depths: # TODO: try using chooseK(train_labels) instead of jus DEFAULT_K\n",
    "        clf_depth = clf.set_params(max_depth = d) # use previous parameters while changing depth\n",
    "\n",
    "        if LOPO:\n",
    "            chosen_cv = LOPO_cv(train_vectors, list_participants_65)\n",
    "        else:\n",
    "            chosen_cv = chooseK(train_labels)\n",
    "\n",
    "        scores = cross_val_score(clf_depth, train_vectors, \n",
    "                                 train_labels, cv = chosen_cv,\n",
    "                                 scoring = 'accuracy') # accuracy here is f1 micro\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "    # changing to misclassification error and determining best depth\n",
    "    MSE = [1 - x for x in cv_scores] # MSE = 1 - f1_micro\n",
    "    optimal_depth = depths[MSE.index(min(MSE))]\n",
    "    print(\"The optimal depth is: \", optimal_depth, \"\\n\")\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", cv_scores[depths.index(optimal_depth)], \"\\n\")\n",
    "    \n",
    "    if plot:\n",
    "        # plot misclassification error vs depths\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        plt.plot(depths, MSE)\n",
    "        plt.xlabel('Tree Depth', fontsize = 20)\n",
    "        plt.ylabel('Misclassification Error', fontsize = 20)\n",
    "        plt.legend(fontsize = 15)\n",
    "        plt.show()\n",
    "\n",
    "    return optimal_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.688034Z",
     "start_time": "2019-06-27T07:13:45.673184Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainKNN(dataframe, list_participants_65=None, LOPO=False):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "    \n",
    "    # split into train and test\n",
    "    test_size_percentage = 0.2 \n",
    "\n",
    "    # X_train = train + cv set (train_vectors)\n",
    "    # X_test = test set (test_vectors)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    parameters = {'n_neighbors' : [3, 5, 7, 9, 10, 11, 12, 13, 14, 15], \n",
    "                  'weights' : ['uniform', 'distance'], \n",
    "                  'metric' : ['seuclidean'], 'algorithm' : ['brute']}\n",
    "    scorer = 'f1_micro'\n",
    "    clf = KNeighborsClassifier(n_neighbors = 3, weights = 'uniform', metric = 'seuclidean', algorithm = 'brute')\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # Leave-one-person-out cross-validation (LOPO)\n",
    "    # --------------------------------------------\n",
    "    if LOPO:\n",
    "        # split into train and test (they still contain the Participant_No as a column)\n",
    "        X_train, X_test, y_train, y_test = LOPOTrainTest(dataframe, list_participants_65, with_labels=True)\n",
    "        knn_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer, \n",
    "                                               list_participants_65=list_participants_65, LOPO=True)\n",
    "        \n",
    "        # X datasets still have the Participant_No, so remove the first column\n",
    "        X_train = X_train[:,1:]\n",
    "        X_test = X_test[:,1:]\n",
    "        \n",
    "    else:\n",
    "        knn_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "\n",
    "    # fitting the model\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(knn_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    knn_acc = getClfMetrics(y_test, y_pred)\n",
    "\n",
    "    return knn_acc, knn_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.705079Z",
     "start_time": "2019-06-27T07:13:45.689135Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainSVM(dataframe):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "    test_size_percentage = 0.2\n",
    "\n",
    "    # X_train = train + cv set (train_vectors)\n",
    "    # X_test = test set (test_vectors)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    parameters = [{'C' : [1, 10, 100, 1000],\n",
    "                   'kernel' : ['linear'], \n",
    "                   'class_weight' : ['balanced']},\n",
    "                  {'C' : [1, 10, 100, 1000], \n",
    "                   'kernel' : ['rbf'], \n",
    "                   'gamma' : [0.1, 0.01, 0.001, 0.0001], \n",
    "                   'class_weight' : ['balanced']}]\n",
    "    clf = SVC(C = 1, kernel = 'linear', class_weight = None, random_state = 100)\n",
    "    scorer = 'f1_micro'\n",
    "    svm_classifier = selectModelParameters(X_train, y_train, clf, parameters, scorer)\n",
    "\n",
    "    # fitting the model\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print(svm_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    svm_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return svm_acc, svm_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.722168Z",
     "start_time": "2019-06-27T07:13:45.706246Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildTrainNB(dataframe):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    print(\"Features: {}\".format(dataframe.columns.values[:-1]))  # minus 1 for the comfort label\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "    test_size_percentage = 0.2\n",
    "\n",
    "    # X_train = train + cv set\n",
    "    # X_test = test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    # instantiate learning model\n",
    "    nb_classifier = GaussianNB() # TODO get priors?\n",
    "\n",
    "    # k-fold cross validation\n",
    "    scores = cross_val_score(nb_classifier, X_train, y_train, cv = DEFAULT_K, scoring = 'accuracy') # accuracy here is f1 micro\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", scores.mean())\n",
    "\n",
    "    # fitting the model\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    print(nb_classifier)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    nb_acc = getClfMetrics(y_test, y_pred)\n",
    "    \n",
    "    return nb_acc , nb_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T07:13:45.740066Z",
     "start_time": "2019-06-27T07:13:45.723335Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildMLP(dataframe, binary=False):\n",
    "    \"\"\"\n",
    "    Breakdown the dataframe into X and y arrays. Later split them in train and test set. Train the model with CV\n",
    "    and report the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_K = 10\n",
    "    \n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(dataframe.iloc[:, 0:dataframe.shape[1] - 1]) # minus 1 for the comfort label\n",
    "    y_original = np.array(dataframe['Discrete Thermal Comfort_TA'])\n",
    "\n",
    "    # re-map labels from {-2, -1, 0, 1, 2} to {0, 1, 2, 3, 4}; works better with _tanh_ activation\n",
    "    y_norm = y_original + 2\n",
    "    \n",
    "    # binary case\n",
    "    if binary:\n",
    "        y_norm = dataframe['Discrete Thermal Comfort_TA'].map(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "    y = np.zeros((y_norm.size, y_norm.max()+1)) \n",
    "    y[np.arange(y_norm.size),y_norm] = 1 \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X) # TODO: does this take care of boolean variables?\n",
    "\n",
    "    # split into train and test\n",
    "    test_size_percentage = 0.2 # standard 80/20 split\n",
    "\n",
    "    # X_train = train + cv set\n",
    "    # X_test = test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size = test_size_percentage, random_state = 100, stratify = y)\n",
    "\n",
    "    # MLP is sensitive to feature scaling. \n",
    "    mlp = MLPClassifier(hidden_layer_sizes = (100,100,25,5), \n",
    "                    activation = 'tanh', \n",
    "                    solver = 'lbfgs',\n",
    "                    learning_rate = 'adaptive',\n",
    "                    alpha = 1e-4, \n",
    "                    learning_rate_init = 1e-3, # only used with solver adam or sgd\n",
    "                    max_iter = 1000,\n",
    "                    tol = 1e-8,\n",
    "                    batch_size = 5,\n",
    "                    random_state = 100,\n",
    "                    verbose = True,\n",
    "                    warm_start = False,\n",
    "                    nesterovs_momentum = True,\n",
    "                    shuffle = True)\n",
    "\n",
    "    # model learned\n",
    "    print(mlp)\n",
    "\n",
    "    # k-fold cross validation\n",
    "    scores = cross_val_score(mlp, X_train, y_train, cv = DEFAULT_K, scoring = 'accuracy') # accuracy here is f1 micro\n",
    "    print(\"Expected accuracy (f1 micro) based on Cross-Validation: \", scores.mean(), \"\\n\")\n",
    "\n",
    "    # fitting the model\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # predict the response\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    # reverse one hot encoding\n",
    "    y_pred_normal = []\n",
    "    y_test_normal = []\n",
    "    \n",
    "    if binary:\n",
    "        for i in range(0,len(y_test)):\n",
    "            if y_pred[i,0] == 1:\n",
    "                y_pred_normal.append(0)\n",
    "            elif y_pred[i,1] == 1:\n",
    "                y_pred_normal.append(1)\n",
    "            else:\n",
    "                y_pred_normal.append(0)\n",
    "                \n",
    "            if y_test[i,0] == 1:\n",
    "                y_test_normal.append(0)\n",
    "            elif y_test[i,1] ==1:\n",
    "                y_test_normal.append(1)\n",
    "            else:\n",
    "                y_test_normal.append(0)\n",
    "        \n",
    "    else: \n",
    "        for i in range(0,len(y_test)):\n",
    "            if y_pred[i,0] == 1:\n",
    "                y_pred_normal.append(-2)\n",
    "            elif y_pred[i,1] == 1:\n",
    "                y_pred_normal.append(-1)\n",
    "            elif y_pred[i,2] == 1:\n",
    "                y_pred_normal.append(0)\n",
    "            elif y_pred[i,3] == 1:\n",
    "                y_pred_normal.append(1)\n",
    "            elif y_pred[i,4] == 1:\n",
    "                y_pred_normal.append(2)\n",
    "            else:\n",
    "                y_pred_normal.append(0)\n",
    "\n",
    "            if y_test[i,0] == 1:\n",
    "                y_test_normal.append(-2)\n",
    "            elif y_test[i,1] ==1:\n",
    "                y_test_normal.append(-1)\n",
    "            elif y_test[i,2] == 1:\n",
    "                y_test_normal.append(0)\n",
    "            elif y_test[i,3] == 1:\n",
    "                y_test_normal.append(1)\n",
    "            elif y_test[i,4] == 1:\n",
    "                y_test_normal.append(2)\n",
    "            else:\n",
    "                y_pred_normal.append(0)\n",
    "    \n",
    "\n",
    "    y_pred_normal = np.array(y_pred_normal)\n",
    "    y_test_normal = np.array(y_test_normal)\n",
    "    \n",
    "    # Metrics\n",
    "    mlp_acc = getClfMetrics(y_test_normal, y_pred_normal)\n",
    "    return mlp_acc, mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T11:20:12.872981Z",
     "start_time": "2019-06-27T11:20:12.866614Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveModel(fileName, model):\n",
    "    pickle.dump(model, open(fileName, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
